{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef GenerateTrainingDataMatrix(rawData, TrainingPercent = 80):\\n    T_len = int(math.ceil(int(len(rawData))* 0.01 * TrainingPercent))\\n    val = T_len + int(math.ceil(len(rawData)*0.01*10))\\n    print(len(rawData))\\n    d2 = rawData[0:T_len,:]\\n    d3 = rawData[T_len+1:val,:]\\n    d4 = rawData[val+1:,:]\\n    return d2,d3,d4\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import itertools\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "import matplotlib.pyplot\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def concatinate_dataset(df , same_pairs):\n",
    "    dic_img_id={}\n",
    "    keys = df.iloc[:,2:]\n",
    "    for x in range(len(df)):\n",
    "        img_id = df['img_id'][x]\n",
    "        dic_img_id[img_id] = keys.iloc[x].values    \n",
    "    new=[]\n",
    "    for x in range(len(same_pairs)):\n",
    "        key1 = same_pairs['img_id_A'][x]\n",
    "        key2 = same_pairs['img_id_B'][x]\n",
    "        data1 = dic_img_id[key1]\n",
    "        data2 = dic_img_id[key2]\n",
    "        concat = np.concatenate((data1,data2), axis=0)\n",
    "        new.append(concat)\n",
    "    new=pd.DataFrame(np.asarray(new))\n",
    "    data=pd.concat([same_pairs, new], axis=1, sort=False)\n",
    "    return data\n",
    "\n",
    "\n",
    "def difference_dataset(df , same_pairs):\n",
    "    dic_img_id={}\n",
    "    keys = df.iloc[:,2:]\n",
    "    for x in range(len(df)):\n",
    "        img_id = df['img_id'][x]\n",
    "        dic_img_id[img_id] = keys.iloc[x].values    \n",
    "    new=[]\n",
    "    for x in range(len(same_pairs)):\n",
    "        key1 = same_pairs['img_id_A'][x]\n",
    "        key2 = same_pairs['img_id_B'][x]\n",
    "        data1 = dic_img_id[key1]\n",
    "        data2 = dic_img_id[key2]\n",
    "        con = data1 - data2\n",
    "        new.append(con)\n",
    "    new=pd.DataFrame(np.asarray(new))\n",
    "    data=pd.concat([same_pairs, new], axis=1, sort=False)\n",
    "    return data\n",
    "\n",
    "def concat_gsc(df,same_pairs):\n",
    "    dic_img_id={}\n",
    "    keys = df.iloc[:,1:]\n",
    "    for x in range(len(df)):\n",
    "        img_id = df['img_id'][x]\n",
    "        dic_img_id[img_id] = keys.iloc[x].values    \n",
    "    new=[]\n",
    "    for x in range(len(same_pairs)):\n",
    "        key1 = same_pairs['img_id_A'][x]\n",
    "        key2 = same_pairs['img_id_B'][x]\n",
    "        data1 = dic_img_id[key1]\n",
    "        data2 = dic_img_id[key2]\n",
    "        concat = np.concatenate((data1,data2), axis=0)\n",
    "        new.append(concat)\n",
    "    new=pd.DataFrame(np.asarray(new))\n",
    "    data=pd.concat([same_pairs, new], axis=1, sort=False)\n",
    "    return data\n",
    "    \n",
    "\n",
    "def difference_dataset_gsc(df , same_pairs):\n",
    "    dic_img_id={}\n",
    "    keys = df.iloc[:,1:]\n",
    "    v=0\n",
    "    for x in range(len(df)):\n",
    "        v=v+1\n",
    "        img_id = df['img_id'][x]  \n",
    "        dic_img_id[img_id] = keys.iloc[x].values   \n",
    "    new=[]\n",
    "   # print(same_pairs['img_id_A'][1])\n",
    "    #print(dic_img_id)\n",
    "    v=0\n",
    "    for x in range(len(same_pairs)):\n",
    "        v=v+1\n",
    "        key1 = same_pairs['img_id_A'][x]\n",
    "        key2 = same_pairs['img_id_B'][x]\n",
    "        data1 = dic_img_id[key1]\n",
    "        data2 = dic_img_id[key2]\n",
    "        con = data1 - data2\n",
    "        new.append(con)\n",
    "    new=pd.DataFrame(np.asarray(new))\n",
    "    data=pd.concat([same_pairs, new], axis=1, sort=False)\n",
    "    return data    \n",
    "    \n",
    "    \n",
    "def get_target_vector(data_frame):\n",
    "    x = data_frame['target'].values\n",
    "    return x.tolist() \n",
    "\n",
    "def generate_raw_data(df1):\n",
    "    x= df1.drop(['img_id_B', 'img_id_A','target'], axis=1)\n",
    "    return x.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfH = pd.read_csv('H:/ml projects/HumanObserved-Dataset/HumanObserved-Dataset/HumanObserved-Features-Data/HumanObserved-Features-Data.csv')\n",
    "same_pairsH = pd.read_csv('H:/ml projects/HumanObserved-Dataset/HumanObserved-Dataset/HumanObserved-Features-Data/same_pairs.csv')\n",
    "diff_pairsH=pd.read_csv('H:/ml projects/HumanObserved-Dataset/HumanObserved-Dataset/HumanObserved-Features-Data/diffn_pairs.csv')\n",
    "\n",
    "dfG=pd.read_csv('H:/ml projects/GSC-Dataset(1)/GSC-Dataset/GSC-Features-Data/GSC-Features.csv')\n",
    "same_pairsG = pd.read_csv('H:/ml projects/GSC-Dataset(1)/GSC-Dataset/GSC-Features-Data/same_pairs.csv')\n",
    "diff_pairsG = pd.read_csv('H:/ml projects/GSC-Dataset(1)/GSC-Dataset/GSC-Features-Data/diffn_pairs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create human data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(diff_pairs)\n",
    "\n",
    "same_human_con = concatinate_dataset(dfH,same_pairsH)\n",
    "diff_human_con = concatinate_dataset(dfH,diff_pairsH)\n",
    "\n",
    "same_human_subtr= difference_dataset(dfH,same_pairsH)\n",
    "diff_human_subtr= difference_dataset(dfH,diff_pairsH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create gsc data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_pairG = diff_pairsG.sample(n=71531)\n",
    "diff_pairG = diff_pairG.reset_index(drop=True)\n",
    "\n",
    "same_gsc_con = concat_gsc(dfG,same_pairsG)\n",
    "diff_gsc_con = concat_gsc(dfG,diff_pairG)\n",
    "\n",
    "same_gsc_subtr= difference_dataset_gsc(dfG,same_pairsG)\n",
    "diff_gsc_subtr= difference_dataset_gsc(dfG,diff_pairG)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# human data ( concatinated  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = diff_human_con.sample(n=791)\n",
    "dat=pd.concat([same_human_con,x], axis=0, sort=False)\n",
    "dat2=dat.sample(frac=1)\n",
    "target_human_concat = get_target_vector(dat2)\n",
    "rawdata_human_concat =generate_raw_data(dat2)\n",
    "#np.savetxt('target_hum_concat.csv', target_human_concat, fmt='%.2f', delimiter=',')\n",
    "#np.savetxt('rawdata_hum_concat.csv', rawdata_human_concat, fmt='%.2f', delimiter=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# human data(difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "x = diff_human_subtr.sample(n=791)\n",
    "dat=pd.concat([same_human_subtr,x], axis=0, sort=False)\n",
    "dat2=dat.sample(frac=1)\n",
    "target_human_subtr = get_target_vector(dat2)\n",
    "rawdata_human_subtr =generate_raw_data(dat2)\n",
    "print(len((rawdata_human_subtr[0])))\n",
    "#np.savetxt('target_hum_subtr.csv', target_human_concat, fmt='%.2f', delimiter=',')\n",
    "#np.savetxt('rawdata_hum_subtr.csv', rawdata_human_concat, fmt='%.2f', delimiter=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gsc data (concatinated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14306, 1015)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same_gsc_cons = same_gsc_con.sample(frac=0.1).reset_index(drop=True)\n",
    "diff_gsc_cons = diff_gsc_con.sample(frac=0.1).reset_index(drop=True)\n",
    "dat=pd.concat([same_gsc_cons,diff_gsc_cons], axis=0, sort=False)\n",
    "dat2=dat.loc[:, (dat != 0).any(axis=0)]\n",
    "dat2=dat2.sample(frac=1)\n",
    "target_gsc_con = get_target_vector(dat2)\n",
    "rawdata_gsc_con =generate_raw_data(dat2)\n",
    "rawdata_gsc_con.shape\n",
    "#np.savetxt('target_gsc_con.csv', target_gsc_con, fmt='%.2f', delimiter=',')\n",
    "#np.savetxt('rawdata_gsc_con.csv', rawdata_gsc_con, fmt='%.2f', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gsc data (difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_gsc_sub = same_gsc_subtr.sample(frac=0.1).reset_index(drop=True)\n",
    "diff_gsc_sub = diff_gsc_subtr.sample(frac=0.1).reset_index(drop=True)\n",
    "dat=pd.concat([same_gsc_sub,diff_gsc_sub], axis=0, sort=False)\n",
    "dat2=dat.loc[:, (dat != 0).any(axis=0)]\n",
    "dat2=dat2.sample(frac=1)\n",
    "\n",
    "\n",
    "#np.savetxt('target_gsc_subtr.csv', target_gsc_subtr, fmt='%.2f', delimiter=',')\n",
    "#np.savetxt('rawdata_gsc_subtr.csv', rawdata_human_subtr, fmt='%.2f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14306, 509)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_gsc_subtr = get_target_vector(dat2)\n",
    "rawdata_gsc_subtr =generate_raw_data(dat2)\n",
    "rawdata_gsc_subtr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14306, 509)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def GenerateTrainingTarget(rawTraining,TrainingPercent = 80):\n",
    "    TrainingLen = int(math.ceil(len(rawTraining)*(TrainingPercent*0.01)))\n",
    "    t           = rawTraining[:TrainingLen]\n",
    "    #print(str(TrainingPercent) + \"% Training Target Generated..\")\n",
    "    return t\n",
    "\n",
    "def GenerateTrainingDataMatrix(rawData, TrainingPercent = 80):\n",
    "    T_len = int(math.ceil(len(rawData[0])*0.01*TrainingPercent))\n",
    "    d2 = rawData[:,0:T_len]\n",
    "    #print(str(TrainingPercent) + \"% Training Data Generated..\")\n",
    "    return d2\n",
    "\n",
    "def GenerateValData(rawData, ValPercent, TrainingCount): \n",
    "    valSize = int(math.ceil(len(rawData[0])*ValPercent*0.01))\n",
    "    V_End = TrainingCount + valSize\n",
    "    dataMatrix = rawData[:,TrainingCount+1:V_End]\n",
    "    #print (str(ValPercent) + \"% Val Data Generated..\")  \n",
    "    return dataMatrix\n",
    "\n",
    "def GenerateValTargetVector(rawData, ValPercent, TrainingCount): \n",
    "    valSize = int(math.ceil(len(rawData)*ValPercent*0.01))\n",
    "    V_End = TrainingCount + valSize\n",
    "    t =rawData[TrainingCount+1:V_End]\n",
    "    #print (str(ValPercent) + \"% Val Target Data Generated..\")\n",
    "    return t\n",
    "\n",
    "def GenerateBigSigma(Data, MuMatrix,TrainingPercent,IsSynthetic):\n",
    "    BigSigma    = np.zeros((len(Data),len(Data)))\n",
    "    DataT       = np.transpose(Data)\n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))        \n",
    "    varVect     = []\n",
    "    for i in range(0,len(DataT[0])):\n",
    "        vct = []\n",
    "        for j in range(0,int(TrainingLen)):\n",
    "            vct.append(Data[i][j])    \n",
    "        varVect.append(np.var(vct))\n",
    "    \n",
    "    for j in range(len(Data)):\n",
    "        BigSigma[j][j] = varVect[j]\n",
    "    if IsSynthetic == True:\n",
    "        BigSigma = np.dot(3,BigSigma)\n",
    "    else:\n",
    "        BigSigma = np.dot(200,BigSigma)\n",
    "    ##print (\"BigSigma Generated..\")\n",
    "    return BigSigma\n",
    "\n",
    "def GetScalar(DataRow,MuRow, BigSigInv):  \n",
    "    R = np.subtract(DataRow,MuRow)\n",
    "    T = np.dot(BigSigInv,np.transpose(R))  \n",
    "    L = np.dot(R,T)\n",
    "    return L\n",
    "\n",
    "def GetRadialBasisOut(DataRow,MuRow, BigSigInv):    \n",
    "    phi_x = math.exp(-0.5*GetScalar(DataRow,MuRow,BigSigInv))\n",
    "    return phi_x\n",
    "\n",
    "def GetPhiMatrix(Data, MuMatrix, BigSigma, TrainingPercent = 80):\n",
    "    DataT = np.transpose(Data)\n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))         \n",
    "    PHI = np.zeros((int(TrainingLen),len(MuMatrix))) \n",
    "    BigSigInv = np.linalg.inv(BigSigma)\n",
    "    for  C in range(0,len(MuMatrix)):\n",
    "        for R in range(0,int(TrainingLen)):\n",
    "            PHI[R][C] = GetRadialBasisOut(DataT[R], MuMatrix[C], BigSigInv)\n",
    "    #print (\"PHI Generated..\")\n",
    "    return PHI\n",
    "\n",
    "def GetWeightsClosedForm(PHI, T, Lambda):\n",
    "    Lambda_I = np.identity(len(PHI[0]))\n",
    "    for i in range(0,len(PHI[0])):\n",
    "        Lambda_I[i][i] = Lambda\n",
    "    PHI_T       = np.transpose(PHI)\n",
    "    PHI_SQR     = np.dot(PHI_T,PHI)\n",
    "    PHI_SQR_LI  = np.add(Lambda_I,PHI_SQR)\n",
    "    PHI_SQR_INV = np.linalg.inv(PHI_SQR_LI)\n",
    "    INTER       = np.dot(PHI_SQR_INV, PHI_T)\n",
    "    W           = np.dot(INTER, T)\n",
    "    ##print (\"Training Weights Generated..\")\n",
    "    return W\n",
    "\n",
    "def GetPhiMatrix(Data, MuMatrix, BigSigma, TrainingPercent = 80):\n",
    "    DataT = np.transpose(Data)\n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))         \n",
    "    PHI = np.zeros((int(TrainingLen),len(MuMatrix))) \n",
    "    BigSigInv = np.linalg.inv(BigSigma)\n",
    "    for  C in range(0,len(MuMatrix)):\n",
    "        for R in range(0,int(TrainingLen)):\n",
    "            PHI[R][C] = GetRadialBasisOut(DataT[R], MuMatrix[C], BigSigInv)\n",
    "    #print (\"PHI Generated..\")\n",
    "    return PHI\n",
    "\n",
    "def GetValTest(VAL_PHI,W):\n",
    "    Y = np.dot(W,np.transpose(VAL_PHI))\n",
    "    ##print (\"Test Out Generated..\")\n",
    "    return Y\n",
    "\n",
    "def GetErms(VAL_TEST_OUT,ValDataAct):\n",
    "    sum = 0.0\n",
    "    t=0\n",
    "    accuracy = 0.0\n",
    "    counter = 0\n",
    "    val = 0.0\n",
    "    for i in range (0,len(VAL_TEST_OUT)):\n",
    "        sum = sum + math.pow((ValDataAct[i] - VAL_TEST_OUT[i]),2)\n",
    "        if(int(np.around(VAL_TEST_OUT[i], 0)) == ValDataAct[i]):\n",
    "            counter+=1\n",
    "    accuracy = (float((counter*100))/float(len(VAL_TEST_OUT)))\n",
    "    ##print (\"Accuracy Generated..\")\n",
    "    ##print (\"Validation E_RMS : \" + str(math.sqrt(sum/len(VAL_TEST_OUT))))\n",
    "    return (str(accuracy) + ',' +  str(math.sqrt(sum/len(VAL_TEST_OUT))))\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid(values):\n",
    "    return 1 / (1 + np.exp(-values))\n",
    "\n",
    "\n",
    "def CostFunction(theta, X, y, _lambda = 0.1):\n",
    "    m = len(y)\n",
    "    h = sigmoid(X.dot(theta))\n",
    "    reg = (_lambda/(2 * m)) * np.sum(theta**2)\n",
    "\n",
    "    return (1 / m) * (-y.T.dot(np.log(h)) - (1 - y).T.dot(np.log(1 - h))) + reg\n",
    "\n",
    "\n",
    "def Gradient(theta, X, y, _lambda ):\n",
    "    m, n = X.shape\n",
    "    theta = theta.reshape((n, 1))\n",
    "    y = y.reshape((m, 1))\n",
    "    h = sigmoid(X.dot(theta))\n",
    "    reg = _lambda * theta /m\n",
    "\n",
    "    return ((1 / m) * X.T.dot(h - y)) + reg\n",
    "\n",
    "\n",
    "rawdata_gsc_subtr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(509, 14306)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print()\n",
    "rawdata_human_subtr= rawdata_human_subtr.T\n",
    "rawdata_gsc_con = rawdata_gsc_con.T\n",
    "rawdata_gsc_subtr = rawdata_gsc_subtr.T\n",
    "rawdata_human_concat =rawdata_human_concat.T\n",
    "rawdata_gsc_subtr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(509, 14306)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(rawdata_gsc_subtr.shape)\n",
    "hum_TrainingTarget_s = np.array(GenerateTrainingTarget(target_human_subtr,80))\n",
    "hum_TrainingData_s   = GenerateTrainingDataMatrix(rawdata_human_subtr,80)\n",
    "\n",
    "hum_TrainingTarget_c = np.array(GenerateTrainingTarget(target_human_concat,80))\n",
    "hum_TrainingData_c   = GenerateTrainingDataMatrix(rawdata_human_concat,80)\n",
    "\n",
    "gsc_TrainingTarget_c =  np.array(GenerateTrainingTarget(target_gsc_con,80))\n",
    "gsc_TrainingData_c   = GenerateTrainingDataMatrix(rawdata_gsc_con,80)\n",
    "\n",
    "gsc_TrainingTarget_s = np.array(GenerateTrainingTarget(target_gsc_subtr,80))\n",
    "gsc_TrainingData_s   = GenerateTrainingDataMatrix(rawdata_gsc_subtr,80)\n",
    "\n",
    "print()\n",
    "\n",
    "X_hs = hum_TrainingData_s.T\n",
    "y_hs = hum_TrainingTarget_s\n",
    "\n",
    "X_hc = hum_TrainingData_c.T\n",
    "y_hc = hum_TrainingTarget_c\n",
    "\n",
    "X_gc = gsc_TrainingData_c.T\n",
    "y_gc = gsc_TrainingTarget_c\n",
    "\n",
    "X_gs = gsc_TrainingData_s.T\n",
    "y_gs = gsc_TrainingTarget_s\n",
    "\n",
    "#print(rawdata_human_subtr.shape)\n",
    "#print((X.shape))\n",
    "def add_bias(X):\n",
    "    nrow=np.shape(X)[0]\n",
    "    bhayas = np.zeros((nrow,1))+5\n",
    "    bhayas = np.ravel(bhayas)\n",
    "    #print(np.shape(bhayas))\n",
    "    bhayas=np.reshape(bhayas,(nrow,1))\n",
    "    X_=np.concatenate((X, bhayas), axis=1)\n",
    "   # print(np.shape(X_))\n",
    "    return X_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1266, 10)\n",
      "(11445, 510)\n",
      "(11445, 1016)\n",
      "(1266, 19)\n"
     ]
    }
   ],
   "source": [
    "X_hs=add_bias(X_hs)\n",
    "X_gs=add_bias(X_gs)\n",
    "X_gc=add_bias(X_gc)\n",
    "X_hc=add_bias(X_hc)\n",
    "print(X_hs.shape)\n",
    "print(X_gs.shape)\n",
    "print(X_gc.shape)\n",
    "print(X_hc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValDataAct_h_s = np.array(GenerateValTargetVector(target_human_subtr,10, (len(hum_TrainingTarget_s))))\n",
    "ValData_h_s    = GenerateValData(rawdata_human_subtr,10, (len(hum_TrainingData_s)))\n",
    "\n",
    "ValDataAct_h_c = np.array(GenerateValTargetVector(target_human_concat,10, (len(hum_TrainingTarget_c))))\n",
    "ValData_h_c    = GenerateValData(rawdata_human_concat,10, (len(hum_TrainingData_c)))\n",
    "\n",
    "\n",
    "ValDataAct_g_c = np.array(GenerateValTargetVector(target_gsc_con,10, (len(gsc_TrainingTarget_c))))\n",
    "ValData_g_c    = GenerateValData(rawdata_gsc_con,10, (len(gsc_TrainingData_c)))\n",
    "\n",
    "\n",
    "ValDataAct_g_s = np.array(GenerateValTargetVector(target_gsc_subtr,10, (len(gsc_TrainingTarget_s))))\n",
    "ValData_g_s    = GenerateValData(rawdata_gsc_subtr,10, (len(gsc_TrainingData_s)))\n",
    "\n",
    "\n",
    "#print(ValData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(509, 1430)\n",
      "(1430,)\n"
     ]
    }
   ],
   "source": [
    "TestDataAct_h_s = np.array(GenerateValTargetVector(target_human_subtr,10, (len(hum_TrainingTarget_s)+len(ValDataAct_h_s))))\n",
    "TestData_h_s = GenerateValData(rawdata_human_subtr,10, (len(hum_TrainingTarget_s)+len(ValDataAct_h_s)))\n",
    "\n",
    "TestDataAct_h_c = np.array(GenerateValTargetVector(target_human_concat,10, (len(hum_TrainingTarget_c)+len(ValDataAct_h_c))))\n",
    "TestData_h_c = GenerateValData(rawdata_human_concat,10, (len(hum_TrainingTarget_c)+len(ValDataAct_h_c)))\n",
    "\n",
    "TestDataAct_g_s = np.array(GenerateValTargetVector(target_gsc_subtr,10, (len(gsc_TrainingTarget_s)+len(ValDataAct_g_s))))\n",
    "TestData_g_s = GenerateValData(rawdata_gsc_subtr,10, (len(gsc_TrainingTarget_s)+len(ValDataAct_g_s)))\n",
    "\n",
    "TestDataAct_g_c = np.array(GenerateValTargetVector(target_gsc_con,10, (len(gsc_TrainingTarget_c)+len(ValDataAct_g_c))))\n",
    "TestData_g_c = GenerateValData(rawdata_gsc_con,10, (len(gsc_TrainingTarget_c)+len(ValDataAct_g_c)))\n",
    "\n",
    "print(TestData_g_s.shape)\n",
    "print(TestDataAct_g_s.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic for human subtreacted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 157)\n",
      "(18, 157)\n",
      "(509, 1430)\n",
      "(1015, 1430)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(TestData_h_s.shape)\n",
    "print(TestData_h_c.shape)\n",
    "print(TestData_g_s.shape)\n",
    "print(TestData_g_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 157)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.46496815286624205"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(TestData_h_s.shape)\n",
    "\n",
    "#print(np.ravel(bhayas))\n",
    "\n",
    "lr =0.01\n",
    "w = np.random.rand(np.shape(X_hs)[1])\n",
    "#w = np.dot(w,220)\n",
    "#print(sigmoid(X.dot(w)))\n",
    "#print(Grad0ient(w, X, y, _lambda = 50))\n",
    "d =0.10\n",
    "for i in range (1000):\n",
    "    delta_w =np.ravel(Gradient(w, X_hs, y_hs, _lambda =d))\n",
    "    w = w - lr*delta_w\n",
    "#print(w)\n",
    "\n",
    "#print(CostFunction(w, X, y, _lambda = d))\n",
    "#print(np.matmul(TestData.T,w.T))\n",
    "b = w[-1]\n",
    "w_ =w[0:len(w)-1]\n",
    "\n",
    "P = sigmoid(w_.dot(TestData_h_s)+b)\n",
    "#print(P.shape)\n",
    "#print(P)co\n",
    "\n",
    "#print(w_)\n",
    "P[P>0.5] = 1 \n",
    "P[P<0.5] = 0\n",
    "accuracy_score(TestDataAct_h_s,P)\n",
    "#print(TestData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic for gsc subtracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5321678321678321"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr =0.05\n",
    "w = np.random.rand(np.shape(X_gs)[1])\n",
    "#w = np.dot(w,220)\n",
    "#print(sigmoid(X.dot(w)))\n",
    "#print(Grad0ient(w, X, y, _lambda = 50))\n",
    "d =0.10\n",
    "for i in range (1000):\n",
    "    delta_w =np.ravel(Gradient(w, X_gs, y_gs, _lambda =d))\n",
    "    w = w - lr*delta_w\n",
    "#print(w)\n",
    "\n",
    "#print(CostFunction(w, X, y, _lambda = d))\n",
    "#print(np.matmul(TestData.T,w.T))\n",
    "b = w[-1]\n",
    "w_ =w[0:len(w)-1]\n",
    "\n",
    "P = sigmoid(w_.dot(TestData_g_s)+b)\n",
    "#print(P.shape)\n",
    "#print(P)\n",
    "\n",
    "#print(w_)\n",
    "P[P>0.5] = 1 \n",
    "P[P<0.5] = 0\n",
    "accuracy_score(TestDataAct_g_s,P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic for human concatinated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4713375796178344"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr =0.01\n",
    "w = np.random.rand(np.shape(X_hc)[1])\n",
    "#w = np.dot(w,220)\n",
    "#print(sigmoid(X.dot(w)))\n",
    "#print(Grad0ient(w, X, y, _lambda = 50))\n",
    "d =0.10\n",
    "for i in range (1000):\n",
    "    delta_w =np.ravel(Gradient(w, X_hc, y_hc, _lambda =d))\n",
    "    w = w - lr*delta_w\n",
    "#print(w)\n",
    "\n",
    "#print(CostFunction(w, X, y, _lambda = d))\n",
    "#print(np.matmul(TestData.T,w.T))\n",
    "b = w[-1]\n",
    "w_ =w[0:len(w)-1]\n",
    "\n",
    "P = sigmoid(w_.dot(TestData_h_c)+b)\n",
    "#print(P.shape)\n",
    "#print(P)\n",
    "\n",
    "#print(w_)\n",
    "P[P>0.5] = 1 \n",
    "P[P<0.5] = 0\n",
    "accuracy_score(TestDataAct_h_c,P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic for gsc concatinated data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.535031847133758"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr =0.01\n",
    "w = np.random.rand(np.shape(X_hs)[1])\n",
    "#w = np.dot(w,220)\n",
    "#print(sigmoid(X.dot(w)))\n",
    "#print(Grad0ient(w, X, y, _lambda = 50))\n",
    "d =0.10\n",
    "for i in range (1000):\n",
    "    delta_w =np.ravel(Gradient(w, X_hs, y_hs, _lambda =d))\n",
    "    w = w - lr*delta_w\n",
    "#print(w)\n",
    "\n",
    "#print(CostFunction(w, X, y, _lambda = d))\n",
    "#print(np.matmul(TestData.T,w.T))\n",
    "b = w[-1]\n",
    "w_ =w[0:len(w)-1]\n",
    "\n",
    "P = sigmoid(w_.dot(TestData_h_s)+b)\n",
    "#print(P.shape)\n",
    "#print(P)\n",
    "\n",
    "#print(w_)\n",
    "P[P>0.5] = 1 \n",
    "P[P<0.5] = 0\n",
    "accuracy_score(TestDataAct_h_s,P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestData_h_s= TestData_h_s.T\n",
    "TestData_h_c = TestData_h_c.T\n",
    "TestData_g_s = TestData_g_s.T\n",
    "TestData_g_c =TestData_g_c.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_23 (Dense)             (None, 1000)              510000    \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 200)               100200    \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1)                 6         \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,111,711\n",
      "Trainable params: 1,111,711\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "import keras\n",
    "import numpy as np\n",
    "import copy\n",
    "#from keras import initializations\n",
    "\n",
    "input_size = 509\n",
    "drop_out = 0.0\n",
    "first_dense_layer_nodes  = 1000\n",
    "second_dense_layer_nodes = 500\n",
    "third = 200\n",
    "fourth =100\n",
    "fifth =35\n",
    "sixth = 5\n",
    "#dropouts = [0.0,0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "result = {}\n",
    "y = {}\n",
    "loss = []\n",
    "acc = []\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(first_dense_layer_nodes, input_dim=input_size))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.add(Dense(second_dense_layer_nodes))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.add(Dense(third))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "#model.add(Dense(fifth))\n",
    "#model.add(Activation('sigmoid'))\n",
    "\n",
    "#model.add(Dense(sixth))\n",
    "#model.add(Activation('sigmoid'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "rms=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon = None,decay=0.0 , amsgrad = False)\n",
    "rms1 =keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "\n",
    "model.compile(optimizer=rms1,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9156 samples, validate on 2289 samples\n",
      "Epoch 1/1000\n",
      "9156/9156 [==============================] - 6s 690us/step - loss: 0.6932 - acc: 0.4998 - val_loss: 0.6936 - val_acc: 0.4836\n",
      "Epoch 2/1000\n",
      "9156/9156 [==============================] - 6s 632us/step - loss: 0.6875 - acc: 0.5475 - val_loss: 0.6757 - val_acc: 0.6339\n",
      "Epoch 3/1000\n",
      "9156/9156 [==============================] - 5s 532us/step - loss: 0.6391 - acc: 0.6761 - val_loss: 0.6329 - val_acc: 0.6846\n",
      "Epoch 4/1000\n",
      "9156/9156 [==============================] - 5s 576us/step - loss: 0.6056 - acc: 0.7125 - val_loss: 0.6097 - val_acc: 0.7029\n",
      "Epoch 5/1000\n",
      "9156/9156 [==============================] - 5s 577us/step - loss: 0.5860 - acc: 0.7246 - val_loss: 0.6150 - val_acc: 0.6763\n",
      "Epoch 6/1000\n",
      "9156/9156 [==============================] - 5s 524us/step - loss: 0.5685 - acc: 0.7363 - val_loss: 0.6367 - val_acc: 0.6444\n",
      "Epoch 7/1000\n",
      "9156/9156 [==============================] - 5s 569us/step - loss: 0.5543 - acc: 0.7464 - val_loss: 0.6021 - val_acc: 0.6920\n",
      "Epoch 8/1000\n",
      "9156/9156 [==============================] - 5s 575us/step - loss: 0.5446 - acc: 0.7505 - val_loss: 0.5908 - val_acc: 0.6986\n",
      "Epoch 9/1000\n",
      "9156/9156 [==============================] - 5s 539us/step - loss: 0.5332 - acc: 0.7612 - val_loss: 0.6040 - val_acc: 0.6933\n",
      "Epoch 10/1000\n",
      "9156/9156 [==============================] - 5s 570us/step - loss: 0.5251 - acc: 0.7646 - val_loss: 0.5937 - val_acc: 0.7016\n",
      "Epoch 11/1000\n",
      "9156/9156 [==============================] - 5s 592us/step - loss: 0.5170 - acc: 0.7729 - val_loss: 0.5923 - val_acc: 0.6994\n",
      "Epoch 12/1000\n",
      "9156/9156 [==============================] - 5s 544us/step - loss: 0.5078 - acc: 0.7791 - val_loss: 0.6365 - val_acc: 0.6798\n",
      "Epoch 13/1000\n",
      "9156/9156 [==============================] - 5s 568us/step - loss: 0.5004 - acc: 0.7815 - val_loss: 0.5714 - val_acc: 0.7208\n",
      "Epoch 14/1000\n",
      "9156/9156 [==============================] - 5s 592us/step - loss: 0.4818 - acc: 0.7945 - val_loss: 0.5640 - val_acc: 0.7226\n",
      "Epoch 15/1000\n",
      "9156/9156 [==============================] - 5s 563us/step - loss: 0.4662 - acc: 0.8044 - val_loss: 0.5738 - val_acc: 0.7256\n",
      "Epoch 16/1000\n",
      "9156/9156 [==============================] - 6s 648us/step - loss: 0.4550 - acc: 0.8105 - val_loss: 0.5517 - val_acc: 0.7396\n",
      "Epoch 17/1000\n",
      "9156/9156 [==============================] - 6s 654us/step - loss: 0.4494 - acc: 0.8174 - val_loss: 0.5563 - val_acc: 0.7335\n",
      "Epoch 18/1000\n",
      "9156/9156 [==============================] - 6s 607us/step - loss: 0.4387 - acc: 0.8227 - val_loss: 0.5716 - val_acc: 0.7361\n",
      "Epoch 19/1000\n",
      "9156/9156 [==============================] - 6s 666us/step - loss: 0.4308 - acc: 0.8280 - val_loss: 0.5701 - val_acc: 0.7379\n",
      "Epoch 20/1000\n",
      "9156/9156 [==============================] - 6s 637us/step - loss: 0.4235 - acc: 0.8349 - val_loss: 0.5792 - val_acc: 0.7278\n",
      "Epoch 21/1000\n",
      "9156/9156 [==============================] - 6s 638us/step - loss: 0.4175 - acc: 0.8344 - val_loss: 0.5845 - val_acc: 0.7344\n",
      "Epoch 00021: early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "validation_data_split = 0.2\n",
    "num_epochs = 1000\n",
    "model_batch_size = 128\n",
    "tb_batch_size = 32\n",
    "early_patience = 5\n",
    "\n",
    "tensorboard_cb   = TensorBoard(log_dir='logs', batch_size= tb_batch_size, write_graph= True)\n",
    "earlystopping_cb = EarlyStopping(monitor='val_loss', verbose=1, patience=early_patience, mode='min') \n",
    "processedData =X_gs[:,0:len(X_gs[0])-1]\n",
    "processedLabel = y_gs\n",
    "\n",
    "history = model.fit(processedData\n",
    "                    , processedLabel\n",
    "                    , validation_split=validation_data_split\n",
    "                    , epochs=num_epochs\n",
    "                    , batch_size=model_batch_size\n",
    "                    , callbacks = [tensorboard_cb,earlystopping_cb]\n",
    "                   )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8230299 ]\n",
      " [0.09732343]\n",
      " [0.09841558]\n",
      " ...\n",
      " [0.69425017]\n",
      " [0.09674435]\n",
      " [0.3931393 ]]\n",
      "[[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(TestData_g_s)\n",
    "print(predictions)\n",
    "predictions[predictions>0.5] = 1 \n",
    "predictions[predictions<0.5]=0\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7391608391608392"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(TestDataAct_g_s,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nueral network gsc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 1000)              1016000   \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               100100    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 5)                 505       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 6         \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,126,711\n",
      "Trainable params: 1,126,711\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_size = 1015\n",
    "drop_out = 0.0\n",
    "first_dense_layer_nodes  = 1000\n",
    "second_dense_layer_nodes = 100\n",
    "third = 100\n",
    "fourth =5\n",
    "#dropouts = [0.0,0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "result = {}\n",
    "y = {}\n",
    "loss = []\n",
    "acc = []\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(first_dense_layer_nodes, input_dim=input_size))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.add(Dense(second_dense_layer_nodes))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.add(Dense(third))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "rms=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon = None,decay=0.0 , amsgrad = False)\n",
    "rms1 =keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "#rmo=keras.optimizers.SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=False)\n",
    "\n",
    "\n",
    "model.compile(optimizer=rms1,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9156 samples, validate on 2289 samples\n",
      "Epoch 1/1000\n",
      "9156/9156 [==============================] - 7s 726us/step - loss: 0.7085 - acc: 0.4971 - val_loss: 0.6931 - val_acc: 0.4980\n",
      "Epoch 2/1000\n",
      "9156/9156 [==============================] - 6s 668us/step - loss: 0.6934 - acc: 0.5004 - val_loss: 0.6933 - val_acc: 0.4980\n",
      "Epoch 3/1000\n",
      "9156/9156 [==============================] - 5s 536us/step - loss: 0.6818 - acc: 0.5463 - val_loss: 0.6687 - val_acc: 0.5845\n",
      "Epoch 4/1000\n",
      "9156/9156 [==============================] - 6s 618us/step - loss: 0.6518 - acc: 0.6041 - val_loss: 0.6508 - val_acc: 0.6016\n",
      "Epoch 5/1000\n",
      "9156/9156 [==============================] - 6s 608us/step - loss: 0.6397 - acc: 0.6204 - val_loss: 0.6558 - val_acc: 0.5793\n",
      "Epoch 6/1000\n",
      "9156/9156 [==============================] - 5s 564us/step - loss: 0.6307 - acc: 0.6287 - val_loss: 0.6491 - val_acc: 0.6099\n",
      "Epoch 7/1000\n",
      "9156/9156 [==============================] - 6s 609us/step - loss: 0.6222 - acc: 0.6341 - val_loss: 0.6489 - val_acc: 0.5928\n",
      "Epoch 8/1000\n",
      "9156/9156 [==============================] - 5s 601us/step - loss: 0.6127 - acc: 0.6424 - val_loss: 0.6974 - val_acc: 0.6029\n",
      "Epoch 9/1000\n",
      "9156/9156 [==============================] - 5s 556us/step - loss: 0.6080 - acc: 0.6468 - val_loss: 0.6501 - val_acc: 0.6107\n",
      "Epoch 10/1000\n",
      "9156/9156 [==============================] - 6s 608us/step - loss: 0.6025 - acc: 0.6524 - val_loss: 0.6477 - val_acc: 0.6038\n",
      "Epoch 11/1000\n",
      "9156/9156 [==============================] - 6s 608us/step - loss: 0.5877 - acc: 0.6690 - val_loss: 0.6559 - val_acc: 0.6225\n",
      "Epoch 12/1000\n",
      "9156/9156 [==============================] - 5s 593us/step - loss: 0.5833 - acc: 0.6727 - val_loss: 0.6413 - val_acc: 0.6243\n",
      "Epoch 13/1000\n",
      "9156/9156 [==============================] - 6s 627us/step - loss: 0.5734 - acc: 0.6834 - val_loss: 0.6427 - val_acc: 0.6466\n",
      "Epoch 14/1000\n",
      "9156/9156 [==============================] - 6s 601us/step - loss: 0.5599 - acc: 0.6965 - val_loss: 0.6326 - val_acc: 0.6557\n",
      "Epoch 15/1000\n",
      "9156/9156 [==============================] - 5s 568us/step - loss: 0.5465 - acc: 0.7153 - val_loss: 0.6732 - val_acc: 0.6592\n",
      "Epoch 16/1000\n",
      "9156/9156 [==============================] - 6s 641us/step - loss: 0.5338 - acc: 0.7263 - val_loss: 0.6270 - val_acc: 0.6789\n",
      "Epoch 17/1000\n",
      "9156/9156 [==============================] - 5s 591us/step - loss: 0.5061 - acc: 0.7551 - val_loss: 0.5756 - val_acc: 0.7304\n",
      "Epoch 18/1000\n",
      "9156/9156 [==============================] - 5s 569us/step - loss: 0.4687 - acc: 0.7936 - val_loss: 0.5642 - val_acc: 0.7099\n",
      "Epoch 19/1000\n",
      "9156/9156 [==============================] - 6s 645us/step - loss: 0.4387 - acc: 0.8140 - val_loss: 0.5849 - val_acc: 0.7523\n",
      "Epoch 20/1000\n",
      "9156/9156 [==============================] - 5s 578us/step - loss: 0.4084 - acc: 0.8327 - val_loss: 0.5353 - val_acc: 0.7619\n",
      "Epoch 21/1000\n",
      "9156/9156 [==============================] - 6s 604us/step - loss: 0.3895 - acc: 0.8452 - val_loss: 0.5301 - val_acc: 0.7431\n",
      "Epoch 22/1000\n",
      "9156/9156 [==============================] - 6s 653us/step - loss: 0.3644 - acc: 0.8575 - val_loss: 0.5283 - val_acc: 0.7510\n",
      "Epoch 23/1000\n",
      "9156/9156 [==============================] - 5s 581us/step - loss: 0.3463 - acc: 0.8670 - val_loss: 0.5247 - val_acc: 0.7549\n",
      "Epoch 24/1000\n",
      "9156/9156 [==============================] - 5s 601us/step - loss: 0.3185 - acc: 0.8820 - val_loss: 0.6126 - val_acc: 0.7641\n",
      "Epoch 25/1000\n",
      "9156/9156 [==============================] - 6s 628us/step - loss: 0.3018 - acc: 0.8925 - val_loss: 0.5804 - val_acc: 0.7763\n",
      "Epoch 26/1000\n",
      "9156/9156 [==============================] - 5s 574us/step - loss: 0.2807 - acc: 0.9051 - val_loss: 0.5981 - val_acc: 0.7750\n",
      "Epoch 27/1000\n",
      "9156/9156 [==============================] - 6s 645us/step - loss: 0.2686 - acc: 0.9080 - val_loss: 0.5388 - val_acc: 0.7837\n",
      "Epoch 28/1000\n",
      "9156/9156 [==============================] - 6s 618us/step - loss: 0.2533 - acc: 0.9160 - val_loss: 0.5396 - val_acc: 0.7877\n",
      "Epoch 00028: early stopping\n"
     ]
    }
   ],
   "source": [
    "validation_data_split = 0.2\n",
    "num_epochs = 1000\n",
    "model_batch_size = 128\n",
    "tb_batch_size = 32\n",
    "early_patience = 5\n",
    "\n",
    "tensorboard_cb   = TensorBoard(log_dir='logs', batch_size= tb_batch_size, write_graph= True)\n",
    "earlystopping_cb = EarlyStopping(monitor='val_loss', verbose=1, patience=early_patience, mode='min') \n",
    "processedData =X_gc[:,0:len(X_gc[0])-1]\n",
    "processedLabel = y_gc\n",
    "\n",
    "history = model.fit(processedData\n",
    "                    , processedLabel\n",
    "                    , validation_split=validation_data_split\n",
    "                    , epochs=num_epochs\n",
    "                    , batch_size=model_batch_size\n",
    "                    , callbacks = [tensorboard_cb,earlystopping_cb]\n",
    "                   )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7923076923076923"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(TestData_g_c)\n",
    "predictions[predictions>0.5] = 1 \n",
    "predictions[predictions<0.5]=0\n",
    "#predictions = model.predict(TestData_g_c)\n",
    "accuracy_score(TestDataAct_g_c,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nueral net for human subtracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_54 (Dense)             (None, 50)                500       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 30)                1530      \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 1)                 31        \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 2,061\n",
      "Trainable params: 2,061\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_size = 9\n",
    "drop_out = 0.0\n",
    "first_dense_layer_nodes  = 50\n",
    "second_dense_layer_nodes = 30\n",
    "third = 10\n",
    "fourth =5\n",
    "#dropouts = [0.0,0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "result = {}\n",
    "y = {}\n",
    "loss = []\n",
    "acc = []\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(first_dense_layer_nodes, input_dim=input_size))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.add(Dense(second_dense_layer_nodes))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "rms=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon = None,decay=0.0 , amsgrad = False)\n",
    "rms1 =keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "rmo=keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.0, nesterov=False)\n",
    "\n",
    "\n",
    "model.compile(optimizer=rmo,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1012 samples, validate on 254 samples\n",
      "Epoch 1/1000\n",
      "1012/1012 [==============================] - 1s 853us/step - loss: 0.6952 - acc: 0.4960 - val_loss: 0.6960 - val_acc: 0.4646\n",
      "Epoch 2/1000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6924 - acc: 0.5188 - val_loss: 0.6948 - val_acc: 0.5276\n",
      "Epoch 3/1000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6930 - acc: 0.5069 - val_loss: 0.6948 - val_acc: 0.5276\n",
      "Epoch 4/1000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6928 - acc: 0.5049 - val_loss: 0.6949 - val_acc: 0.4724\n",
      "Epoch 5/1000\n",
      "1012/1012 [==============================] - 0s 19us/step - loss: 0.6932 - acc: 0.4931 - val_loss: 0.6957 - val_acc: 0.4685\n",
      "Epoch 6/1000\n",
      "1012/1012 [==============================] - 0s 22us/step - loss: 0.6927 - acc: 0.5099 - val_loss: 0.6952 - val_acc: 0.4449\n",
      "Epoch 7/1000\n",
      "1012/1012 [==============================] - 0s 21us/step - loss: 0.6933 - acc: 0.4931 - val_loss: 0.6948 - val_acc: 0.5276\n",
      "Epoch 8/1000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6927 - acc: 0.5079 - val_loss: 0.6948 - val_acc: 0.4843\n",
      "Epoch 9/1000\n",
      "1012/1012 [==============================] - 0s 34us/step - loss: 0.6928 - acc: 0.5148 - val_loss: 0.6954 - val_acc: 0.4606\n",
      "Epoch 10/1000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6934 - acc: 0.5069 - val_loss: 0.6948 - val_acc: 0.4764\n",
      "Epoch 11/1000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6926 - acc: 0.5099 - val_loss: 0.6948 - val_acc: 0.4921\n",
      "Epoch 12/1000\n",
      "1012/1012 [==============================] - 0s 22us/step - loss: 0.6932 - acc: 0.4852 - val_loss: 0.6959 - val_acc: 0.4724\n",
      "Epoch 13/1000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6927 - acc: 0.5138 - val_loss: 0.6949 - val_acc: 0.4646\n",
      "Epoch 14/1000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6922 - acc: 0.5040 - val_loss: 0.6947 - val_acc: 0.4961\n",
      "Epoch 15/1000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6922 - acc: 0.5089 - val_loss: 0.6948 - val_acc: 0.4764\n",
      "Epoch 16/1000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6929 - acc: 0.5128 - val_loss: 0.6952 - val_acc: 0.4646\n",
      "Epoch 17/1000\n",
      "1012/1012 [==============================] - 0s 33us/step - loss: 0.6925 - acc: 0.5020 - val_loss: 0.6947 - val_acc: 0.4882\n",
      "Epoch 18/1000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6924 - acc: 0.5069 - val_loss: 0.6950 - val_acc: 0.4528\n",
      "Epoch 19/1000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6930 - acc: 0.4960 - val_loss: 0.6946 - val_acc: 0.4843\n",
      "Epoch 20/1000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6920 - acc: 0.5089 - val_loss: 0.6948 - val_acc: 0.4606\n",
      "Epoch 21/1000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6921 - acc: 0.5049 - val_loss: 0.6950 - val_acc: 0.4409\n",
      "Epoch 22/1000\n",
      "1012/1012 [==============================] - 0s 32us/step - loss: 0.6923 - acc: 0.5257 - val_loss: 0.6952 - val_acc: 0.4646\n",
      "Epoch 23/1000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6919 - acc: 0.5208 - val_loss: 0.6947 - val_acc: 0.4843\n",
      "Epoch 24/1000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6919 - acc: 0.5119 - val_loss: 0.6947 - val_acc: 0.4843\n",
      "Epoch 25/1000\n",
      "1012/1012 [==============================] - 0s 32us/step - loss: 0.6919 - acc: 0.5049 - val_loss: 0.6947 - val_acc: 0.4724\n",
      "Epoch 26/1000\n",
      "1012/1012 [==============================] - 0s 18us/step - loss: 0.6918 - acc: 0.5128 - val_loss: 0.6949 - val_acc: 0.4567\n",
      "Epoch 27/1000\n",
      "1012/1012 [==============================] - 0s 15us/step - loss: 0.6920 - acc: 0.5188 - val_loss: 0.6950 - val_acc: 0.4528\n",
      "Epoch 28/1000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6919 - acc: 0.5128 - val_loss: 0.6947 - val_acc: 0.4803\n",
      "Epoch 29/1000\n",
      "1012/1012 [==============================] - 0s 34us/step - loss: 0.6918 - acc: 0.5069 - val_loss: 0.6946 - val_acc: 0.4843\n",
      "Epoch 30/1000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6919 - acc: 0.5119 - val_loss: 0.6949 - val_acc: 0.4606\n",
      "Epoch 31/1000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6918 - acc: 0.5208 - val_loss: 0.6950 - val_acc: 0.4528\n",
      "Epoch 32/1000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6919 - acc: 0.5069 - val_loss: 0.6946 - val_acc: 0.4764\n",
      "Epoch 33/1000\n",
      "1012/1012 [==============================] - 0s 22us/step - loss: 0.6918 - acc: 0.5049 - val_loss: 0.6947 - val_acc: 0.4488\n",
      "Epoch 34/1000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6920 - acc: 0.5188 - val_loss: 0.6952 - val_acc: 0.4882\n",
      "Epoch 35/1000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6920 - acc: 0.5089 - val_loss: 0.6946 - val_acc: 0.4803\n",
      "Epoch 36/1000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6917 - acc: 0.5010 - val_loss: 0.6946 - val_acc: 0.4646\n",
      "Epoch 37/1000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6918 - acc: 0.5287 - val_loss: 0.6949 - val_acc: 0.4567\n",
      "Epoch 38/1000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6917 - acc: 0.5109 - val_loss: 0.6947 - val_acc: 0.4567\n",
      "Epoch 39/1000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6918 - acc: 0.5030 - val_loss: 0.6946 - val_acc: 0.4764\n",
      "Epoch 40/1000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6916 - acc: 0.5089 - val_loss: 0.6950 - val_acc: 0.4567\n",
      "Epoch 41/1000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6916 - acc: 0.5128 - val_loss: 0.6948 - val_acc: 0.4606\n",
      "Epoch 42/1000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6916 - acc: 0.5049 - val_loss: 0.6946 - val_acc: 0.4803\n",
      "Epoch 43/1000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6914 - acc: 0.5020 - val_loss: 0.6947 - val_acc: 0.4724\n",
      "Epoch 44/1000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6914 - acc: 0.5217 - val_loss: 0.6949 - val_acc: 0.4606\n",
      "Epoch 45/1000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6915 - acc: 0.5237 - val_loss: 0.6949 - val_acc: 0.4606\n",
      "Epoch 46/1000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6915 - acc: 0.5119 - val_loss: 0.6945 - val_acc: 0.5039\n",
      "Epoch 47/1000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6915 - acc: 0.5010 - val_loss: 0.6945 - val_acc: 0.5079\n",
      "Epoch 48/1000\n",
      "1012/1012 [==============================] - 0s 31us/step - loss: 0.6919 - acc: 0.5079 - val_loss: 0.6955 - val_acc: 0.4843\n",
      "Epoch 49/1000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6916 - acc: 0.5306 - val_loss: 0.6951 - val_acc: 0.4803\n",
      "Epoch 50/1000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6913 - acc: 0.5099 - val_loss: 0.6945 - val_acc: 0.5079\n",
      "Epoch 51/1000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6914 - acc: 0.5049 - val_loss: 0.6945 - val_acc: 0.5079\n",
      "Epoch 52/1000\n",
      "1012/1012 [==============================] - 0s 20us/step - loss: 0.6913 - acc: 0.5079 - val_loss: 0.6946 - val_acc: 0.4685\n",
      "Epoch 53/1000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6915 - acc: 0.5079 - val_loss: 0.6945 - val_acc: 0.4646\n",
      "Epoch 54/1000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6917 - acc: 0.5227 - val_loss: 0.6953 - val_acc: 0.4724\n",
      "Epoch 55/1000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6913 - acc: 0.5336 - val_loss: 0.6949 - val_acc: 0.4724\n",
      "Epoch 56/1000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6915 - acc: 0.5168 - val_loss: 0.6945 - val_acc: 0.5157\n",
      "Epoch 57/1000\n",
      "1012/1012 [==============================] - 0s 22us/step - loss: 0.6912 - acc: 0.5030 - val_loss: 0.6945 - val_acc: 0.4685\n",
      "Epoch 58/1000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6913 - acc: 0.5089 - val_loss: 0.6950 - val_acc: 0.4843\n",
      "Epoch 59/1000\n",
      "1012/1012 [==============================] - 0s 32us/step - loss: 0.6913 - acc: 0.5208 - val_loss: 0.6946 - val_acc: 0.4646\n",
      "Epoch 60/1000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6912 - acc: 0.5010 - val_loss: 0.6945 - val_acc: 0.4921\n",
      "Epoch 61/1000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6919 - acc: 0.5188 - val_loss: 0.6954 - val_acc: 0.4843\n",
      "Epoch 62/1000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6912 - acc: 0.5277 - val_loss: 0.6947 - val_acc: 0.4724\n",
      "Epoch 63/1000\n",
      "1012/1012 [==============================] - 0s 21us/step - loss: 0.6913 - acc: 0.5168 - val_loss: 0.6945 - val_acc: 0.5039\n",
      "Epoch 64/1000\n",
      "1012/1012 [==============================] - 0s 22us/step - loss: 0.6914 - acc: 0.5040 - val_loss: 0.6947 - val_acc: 0.4764\n",
      "Epoch 65/1000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6912 - acc: 0.5247 - val_loss: 0.6948 - val_acc: 0.4843\n",
      "Epoch 66/1000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6909 - acc: 0.5287 - val_loss: 0.6946 - val_acc: 0.4764\n",
      "Epoch 67/1000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6912 - acc: 0.5109 - val_loss: 0.6946 - val_acc: 0.4764\n",
      "Epoch 68/1000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6908 - acc: 0.5217 - val_loss: 0.6946 - val_acc: 0.4685\n",
      "Epoch 69/1000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6909 - acc: 0.5138 - val_loss: 0.6945 - val_acc: 0.4961\n",
      "Epoch 70/1000\n",
      "1012/1012 [==============================] - 0s 19us/step - loss: 0.6910 - acc: 0.5040 - val_loss: 0.6946 - val_acc: 0.4685\n",
      "Epoch 71/1000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6912 - acc: 0.4931 - val_loss: 0.6946 - val_acc: 0.4764\n",
      "Epoch 72/1000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6908 - acc: 0.5138 - val_loss: 0.6947 - val_acc: 0.4843\n",
      "Epoch 73/1000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6910 - acc: 0.5306 - val_loss: 0.6950 - val_acc: 0.4882\n",
      "Epoch 74/1000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6919 - acc: 0.5079 - val_loss: 0.6944 - val_acc: 0.5039\n",
      "Epoch 75/1000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6907 - acc: 0.5069 - val_loss: 0.6946 - val_acc: 0.4803\n",
      "Epoch 76/1000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6909 - acc: 0.5316 - val_loss: 0.6950 - val_acc: 0.4882\n",
      "Epoch 77/1000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6909 - acc: 0.5237 - val_loss: 0.6946 - val_acc: 0.4724\n",
      "Epoch 78/1000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6910 - acc: 0.5326 - val_loss: 0.6948 - val_acc: 0.4724\n",
      "Epoch 79/1000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6910 - acc: 0.5040 - val_loss: 0.6945 - val_acc: 0.4803\n",
      "Epoch 80/1000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6908 - acc: 0.5059 - val_loss: 0.6945 - val_acc: 0.4921\n",
      "Epoch 81/1000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6910 - acc: 0.5217 - val_loss: 0.6950 - val_acc: 0.4961\n",
      "Epoch 82/1000\n",
      "1012/1012 [==============================] - 0s 20us/step - loss: 0.6910 - acc: 0.5168 - val_loss: 0.6945 - val_acc: 0.4882\n",
      "Epoch 83/1000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6912 - acc: 0.5089 - val_loss: 0.6944 - val_acc: 0.5000\n",
      "Epoch 84/1000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6909 - acc: 0.5158 - val_loss: 0.6951 - val_acc: 0.4961\n",
      "Epoch 85/1000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6909 - acc: 0.5395 - val_loss: 0.6952 - val_acc: 0.4921\n",
      "Epoch 86/1000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6906 - acc: 0.5178 - val_loss: 0.6944 - val_acc: 0.5039\n",
      "Epoch 87/1000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6910 - acc: 0.4970 - val_loss: 0.6944 - val_acc: 0.5079\n",
      "Epoch 88/1000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6906 - acc: 0.5040 - val_loss: 0.6948 - val_acc: 0.4961\n",
      "Epoch 89/1000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6905 - acc: 0.5346 - val_loss: 0.6948 - val_acc: 0.4882\n",
      "Epoch 90/1000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6906 - acc: 0.5366 - val_loss: 0.6949 - val_acc: 0.4921\n",
      "Epoch 91/1000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6908 - acc: 0.5099 - val_loss: 0.6944 - val_acc: 0.5000\n",
      "Epoch 92/1000\n",
      "1012/1012 [==============================] - 0s 31us/step - loss: 0.6905 - acc: 0.5049 - val_loss: 0.6945 - val_acc: 0.4921\n",
      "Epoch 93/1000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6905 - acc: 0.5198 - val_loss: 0.6948 - val_acc: 0.4921\n",
      "Epoch 94/1000\n",
      "1012/1012 [==============================] - 0s 34us/step - loss: 0.6904 - acc: 0.5277 - val_loss: 0.6947 - val_acc: 0.4803\n",
      "Epoch 95/1000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6906 - acc: 0.5287 - val_loss: 0.6947 - val_acc: 0.4803\n",
      "Epoch 96/1000\n",
      "1012/1012 [==============================] - 0s 22us/step - loss: 0.6907 - acc: 0.4990 - val_loss: 0.6944 - val_acc: 0.5000\n",
      "Epoch 97/1000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6908 - acc: 0.5010 - val_loss: 0.6944 - val_acc: 0.4882\n",
      "Epoch 98/1000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6908 - acc: 0.5198 - val_loss: 0.6949 - val_acc: 0.4961\n",
      "Epoch 99/1000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6904 - acc: 0.5366 - val_loss: 0.6948 - val_acc: 0.4961\n",
      "Epoch 100/1000\n",
      "1012/1012 [==============================] - 0s 31us/step - loss: 0.6904 - acc: 0.5415 - val_loss: 0.6947 - val_acc: 0.4764\n",
      "Epoch 101/1000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6904 - acc: 0.5099 - val_loss: 0.6945 - val_acc: 0.5079\n",
      "Epoch 102/1000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6913 - acc: 0.5010 - val_loss: 0.6944 - val_acc: 0.5197\n",
      "Epoch 103/1000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6904 - acc: 0.5267 - val_loss: 0.6951 - val_acc: 0.4961\n",
      "Epoch 104/1000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6906 - acc: 0.5247 - val_loss: 0.6947 - val_acc: 0.4882\n",
      "Epoch 105/1000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6905 - acc: 0.5306 - val_loss: 0.6947 - val_acc: 0.4843\n",
      "Epoch 106/1000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6907 - acc: 0.5148 - val_loss: 0.6945 - val_acc: 0.5079\n",
      "Epoch 107/1000\n",
      "1012/1012 [==============================] - 0s 34us/step - loss: 0.6902 - acc: 0.5188 - val_loss: 0.6948 - val_acc: 0.5000\n",
      "Epoch 108/1000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6904 - acc: 0.5316 - val_loss: 0.6947 - val_acc: 0.4882\n",
      "Epoch 109/1000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6902 - acc: 0.5217 - val_loss: 0.6947 - val_acc: 0.4843\n",
      "Epoch 110/1000\n",
      "1012/1012 [==============================] - 0s 21us/step - loss: 0.6907 - acc: 0.5010 - val_loss: 0.6944 - val_acc: 0.5039\n",
      "Epoch 111/1000\n",
      "1012/1012 [==============================] - 0s 34us/step - loss: 0.6905 - acc: 0.5178 - val_loss: 0.6949 - val_acc: 0.4882\n",
      "Epoch 112/1000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6902 - acc: 0.5435 - val_loss: 0.6948 - val_acc: 0.5000\n",
      "Epoch 113/1000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6903 - acc: 0.5415 - val_loss: 0.6947 - val_acc: 0.4921\n",
      "Epoch 114/1000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6900 - acc: 0.5128 - val_loss: 0.6945 - val_acc: 0.5118\n",
      "Epoch 115/1000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6904 - acc: 0.4980 - val_loss: 0.6944 - val_acc: 0.4921\n",
      "Epoch 116/1000\n",
      "1012/1012 [==============================] - 0s 21us/step - loss: 0.6902 - acc: 0.5158 - val_loss: 0.6945 - val_acc: 0.5236\n",
      "Epoch 117/1000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6903 - acc: 0.5356 - val_loss: 0.6950 - val_acc: 0.4921\n",
      "Epoch 118/1000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6901 - acc: 0.5415 - val_loss: 0.6948 - val_acc: 0.5039\n",
      "Epoch 119/1000\n",
      "1012/1012 [==============================] - 0s 33us/step - loss: 0.6900 - acc: 0.5385 - val_loss: 0.6946 - val_acc: 0.5039\n",
      "Epoch 120/1000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6904 - acc: 0.5109 - val_loss: 0.6944 - val_acc: 0.5000\n",
      "Epoch 121/1000\n",
      "1012/1012 [==============================] - 0s 22us/step - loss: 0.6904 - acc: 0.5128 - val_loss: 0.6947 - val_acc: 0.5000\n",
      "Epoch 122/1000\n",
      "1012/1012 [==============================] - 0s 20us/step - loss: 0.6902 - acc: 0.5395 - val_loss: 0.6948 - val_acc: 0.5000\n",
      "Epoch 123/1000\n",
      "1012/1012 [==============================] - 0s 19us/step - loss: 0.6906 - acc: 0.5168 - val_loss: 0.6944 - val_acc: 0.4921\n",
      "Epoch 124/1000\n",
      "1012/1012 [==============================] - 0s 21us/step - loss: 0.6902 - acc: 0.5119 - val_loss: 0.6944 - val_acc: 0.5118\n",
      "Epoch 125/1000\n",
      "1012/1012 [==============================] - 0s 32us/step - loss: 0.6903 - acc: 0.5326 - val_loss: 0.6955 - val_acc: 0.4646\n",
      "Epoch 126/1000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6902 - acc: 0.5415 - val_loss: 0.6947 - val_acc: 0.5079\n",
      "Epoch 127/1000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6899 - acc: 0.5168 - val_loss: 0.6945 - val_acc: 0.5236\n",
      "Epoch 128/1000\n",
      "1012/1012 [==============================] - 0s 22us/step - loss: 0.6911 - acc: 0.5217 - val_loss: 0.6950 - val_acc: 0.5000\n",
      "Epoch 129/1000\n",
      "1012/1012 [==============================] - 0s 35us/step - loss: 0.6899 - acc: 0.5326 - val_loss: 0.6944 - val_acc: 0.4961\n",
      "Epoch 130/1000\n",
      "1012/1012 [==============================] - 0s 31us/step - loss: 0.6904 - acc: 0.5069 - val_loss: 0.6944 - val_acc: 0.5000\n",
      "Epoch 131/1000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6900 - acc: 0.5049 - val_loss: 0.6946 - val_acc: 0.5157\n",
      "Epoch 132/1000\n",
      "1012/1012 [==============================] - 0s 32us/step - loss: 0.6901 - acc: 0.5316 - val_loss: 0.6949 - val_acc: 0.4882\n",
      "Epoch 133/1000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6901 - acc: 0.5247 - val_loss: 0.6945 - val_acc: 0.5276\n",
      "Epoch 134/1000\n",
      "1012/1012 [==============================] - 0s 21us/step - loss: 0.6901 - acc: 0.5287 - val_loss: 0.6946 - val_acc: 0.5079\n",
      "Epoch 135/1000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6902 - acc: 0.5385 - val_loss: 0.6949 - val_acc: 0.4882\n",
      "Epoch 136/1000\n",
      "1012/1012 [==============================] - 0s 33us/step - loss: 0.6900 - acc: 0.5208 - val_loss: 0.6944 - val_acc: 0.4961\n",
      "Epoch 137/1000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6900 - acc: 0.5000 - val_loss: 0.6945 - val_acc: 0.5236\n",
      "Epoch 138/1000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6899 - acc: 0.5158 - val_loss: 0.6947 - val_acc: 0.5039\n",
      "Epoch 139/1000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6901 - acc: 0.5237 - val_loss: 0.6948 - val_acc: 0.4961\n",
      "Epoch 140/1000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6899 - acc: 0.5247 - val_loss: 0.6944 - val_acc: 0.5118\n",
      "Epoch 141/1000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6899 - acc: 0.5119 - val_loss: 0.6944 - val_acc: 0.5079\n",
      "Epoch 142/1000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6899 - acc: 0.5148 - val_loss: 0.6947 - val_acc: 0.5000\n",
      "Epoch 143/1000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6900 - acc: 0.5148 - val_loss: 0.6946 - val_acc: 0.5157\n",
      "Epoch 144/1000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6901 - acc: 0.5336 - val_loss: 0.6950 - val_acc: 0.4843\n",
      "Epoch 145/1000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6900 - acc: 0.5237 - val_loss: 0.6945 - val_acc: 0.5197\n",
      "Epoch 146/1000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6898 - acc: 0.5178 - val_loss: 0.6946 - val_acc: 0.5197\n",
      "Epoch 147/1000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6899 - acc: 0.5277 - val_loss: 0.6946 - val_acc: 0.5157\n",
      "Epoch 148/1000\n",
      "1012/1012 [==============================] - 0s 34us/step - loss: 0.6896 - acc: 0.5128 - val_loss: 0.6944 - val_acc: 0.5157\n",
      "Epoch 149/1000\n",
      "1012/1012 [==============================] - 0s 35us/step - loss: 0.6898 - acc: 0.5089 - val_loss: 0.6945 - val_acc: 0.5315\n",
      "Epoch 150/1000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6900 - acc: 0.5217 - val_loss: 0.6945 - val_acc: 0.5276\n",
      "Epoch 151/1000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6899 - acc: 0.5030 - val_loss: 0.6944 - val_acc: 0.5157\n",
      "Epoch 152/1000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6897 - acc: 0.5128 - val_loss: 0.6946 - val_acc: 0.5197\n",
      "Epoch 153/1000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6896 - acc: 0.5237 - val_loss: 0.6948 - val_acc: 0.4961\n",
      "Epoch 154/1000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6897 - acc: 0.5425 - val_loss: 0.6948 - val_acc: 0.4921\n",
      "Epoch 155/1000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6896 - acc: 0.5267 - val_loss: 0.6945 - val_acc: 0.5315\n",
      "Epoch 156/1000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6897 - acc: 0.5079 - val_loss: 0.6945 - val_acc: 0.5354\n",
      "Epoch 157/1000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6897 - acc: 0.5247 - val_loss: 0.6947 - val_acc: 0.5000\n",
      "Epoch 158/1000\n",
      "1012/1012 [==============================] - 0s 36us/step - loss: 0.6899 - acc: 0.5079 - val_loss: 0.6945 - val_acc: 0.5276\n",
      "Epoch 00158: early stopping\n"
     ]
    }
   ],
   "source": [
    "validation_data_split = 0.2\n",
    "num_epochs = 1000\n",
    "model_batch_size = 128\n",
    "tb_batch_size = 32\n",
    "early_patience = 35\n",
    "\n",
    "tensorboard_cb   = TensorBoard(log_dir='logs', batch_size= tb_batch_size, write_graph= True)\n",
    "earlystopping_cb = EarlyStopping(monitor='val_loss', verbose=1, patience=early_patience, mode='min') \n",
    "processedData =X_hs[:,0:len(X_hs[0])-1]\n",
    "processedLabel = y_hs\n",
    "\n",
    "history = model.fit(processedData\n",
    "                    , processedLabel\n",
    "                    , validation_split=validation_data_split\n",
    "                    , epochs=num_epochs\n",
    "                    , batch_size=model_batch_size\n",
    "                    , callbacks = [tensorboard_cb,earlystopping_cb]\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4840764331210191"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(TestData_h_s)\n",
    "predictions[predictions>0.5] = 1 \n",
    "predictions[predictions<0.5]=0\n",
    "#predictions = model.predict(TestData_g_c)\n",
    "accuracy_score(TestDataAct_h_s,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_57 (Dense)             (None, 50)                950       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 30)                1530      \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 1)                 31        \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 2,511\n",
      "Trainable params: 2,511\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_size = 18\n",
    "drop_out = 0.0\n",
    "first_dense_layer_nodes  = 50\n",
    "second_dense_layer_nodes = 30\n",
    "third = 10\n",
    "fourth =5\n",
    "#dropouts = [0.0,0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "result = {}\n",
    "y = {}\n",
    "loss = []\n",
    "acc = []\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(first_dense_layer_nodes, input_dim=input_size))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.add(Dense(second_dense_layer_nodes))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "rms=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon = None,decay=0.0 , amsgrad = False)\n",
    "rms1 =keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "#rmo=keras.optimizers.SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=False)\n",
    "\n",
    "\n",
    "model.compile(optimizer=rms,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1012 samples, validate on 254 samples\n",
      "Epoch 1/1000\n",
      "1012/1012 [==============================] - 1s 1ms/step - loss: 0.6997 - acc: 0.4990 - val_loss: 0.6917 - val_acc: 0.5354\n",
      "Epoch 2/1000\n",
      "1012/1012 [==============================] - 0s 37us/step - loss: 0.6940 - acc: 0.5099 - val_loss: 0.7002 - val_acc: 0.4646\n",
      "Epoch 3/1000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6946 - acc: 0.5138 - val_loss: 0.6945 - val_acc: 0.4764\n",
      "Epoch 4/1000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6927 - acc: 0.5099 - val_loss: 0.6912 - val_acc: 0.5591\n",
      "Epoch 5/1000\n",
      "1012/1012 [==============================] - 0s 49us/step - loss: 0.6927 - acc: 0.5188 - val_loss: 0.6908 - val_acc: 0.5591\n",
      "Epoch 6/1000\n",
      "1012/1012 [==============================] - 0s 33us/step - loss: 0.6920 - acc: 0.5277 - val_loss: 0.6931 - val_acc: 0.5079\n",
      "Epoch 7/1000\n",
      "1012/1012 [==============================] - 0s 31us/step - loss: 0.6917 - acc: 0.5178 - val_loss: 0.6929 - val_acc: 0.5079\n",
      "Epoch 8/1000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6911 - acc: 0.5217 - val_loss: 0.6933 - val_acc: 0.5079\n",
      "Epoch 9/1000\n",
      "1012/1012 [==============================] - 0s 32us/step - loss: 0.6906 - acc: 0.5375 - val_loss: 0.6912 - val_acc: 0.5630\n",
      "Epoch 10/1000\n",
      "1012/1012 [==============================] - 0s 33us/step - loss: 0.6903 - acc: 0.5613 - val_loss: 0.6910 - val_acc: 0.5591\n",
      "Epoch 11/1000\n",
      "1012/1012 [==============================] - 0s 33us/step - loss: 0.6896 - acc: 0.5603 - val_loss: 0.6925 - val_acc: 0.5354\n",
      "Epoch 12/1000\n",
      "1012/1012 [==============================] - 0s 32us/step - loss: 0.6893 - acc: 0.5543 - val_loss: 0.6931 - val_acc: 0.5276\n",
      "Epoch 13/1000\n",
      "1012/1012 [==============================] - 0s 33us/step - loss: 0.6903 - acc: 0.5158 - val_loss: 0.6952 - val_acc: 0.5079\n",
      "Epoch 14/1000\n",
      "1012/1012 [==============================] - 0s 33us/step - loss: 0.6884 - acc: 0.5553 - val_loss: 0.6901 - val_acc: 0.5276\n",
      "Epoch 15/1000\n",
      "1012/1012 [==============================] - 0s 32us/step - loss: 0.6879 - acc: 0.5563 - val_loss: 0.6905 - val_acc: 0.5394\n",
      "Epoch 16/1000\n",
      "1012/1012 [==============================] - 0s 36us/step - loss: 0.6876 - acc: 0.5514 - val_loss: 0.6916 - val_acc: 0.5472\n",
      "Epoch 17/1000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6870 - acc: 0.5603 - val_loss: 0.6937 - val_acc: 0.4961\n",
      "Epoch 18/1000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6866 - acc: 0.5603 - val_loss: 0.6923 - val_acc: 0.5433\n",
      "Epoch 19/1000\n",
      "1012/1012 [==============================] - 0s 34us/step - loss: 0.6861 - acc: 0.5623 - val_loss: 0.6930 - val_acc: 0.5276\n",
      "Epoch 20/1000\n",
      "1012/1012 [==============================] - 0s 66us/step - loss: 0.6855 - acc: 0.5573 - val_loss: 0.6919 - val_acc: 0.5630\n",
      "Epoch 21/1000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6856 - acc: 0.5692 - val_loss: 0.6894 - val_acc: 0.5394\n",
      "Epoch 22/1000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6847 - acc: 0.5632 - val_loss: 0.6921 - val_acc: 0.5512\n",
      "Epoch 23/1000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6848 - acc: 0.5524 - val_loss: 0.6973 - val_acc: 0.4921\n",
      "Epoch 24/1000\n",
      "1012/1012 [==============================] - 0s 31us/step - loss: 0.6840 - acc: 0.5583 - val_loss: 0.6933 - val_acc: 0.5472\n",
      "Epoch 25/1000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6836 - acc: 0.5771 - val_loss: 0.6898 - val_acc: 0.5433\n",
      "Epoch 26/1000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6839 - acc: 0.5613 - val_loss: 0.6940 - val_acc: 0.5472\n",
      "Epoch 27/1000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6826 - acc: 0.5593 - val_loss: 0.6930 - val_acc: 0.5551\n",
      "Epoch 28/1000\n",
      "1012/1012 [==============================] - 0s 32us/step - loss: 0.6822 - acc: 0.5573 - val_loss: 0.6937 - val_acc: 0.5433\n",
      "Epoch 29/1000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6819 - acc: 0.5652 - val_loss: 0.6916 - val_acc: 0.5512\n",
      "Epoch 30/1000\n",
      "1012/1012 [==============================] - 0s 47us/step - loss: 0.6817 - acc: 0.5672 - val_loss: 0.6938 - val_acc: 0.5591\n",
      "Epoch 31/1000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6816 - acc: 0.5731 - val_loss: 0.6947 - val_acc: 0.5591\n",
      "Epoch 32/1000\n",
      "1012/1012 [==============================] - 0s 33us/step - loss: 0.6807 - acc: 0.5692 - val_loss: 0.6951 - val_acc: 0.5630\n",
      "Epoch 33/1000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6803 - acc: 0.5632 - val_loss: 0.6930 - val_acc: 0.5472\n",
      "Epoch 34/1000\n",
      "1012/1012 [==============================] - 0s 34us/step - loss: 0.6802 - acc: 0.5682 - val_loss: 0.6933 - val_acc: 0.5512\n",
      "Epoch 35/1000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6802 - acc: 0.5682 - val_loss: 0.6946 - val_acc: 0.5591\n",
      "Epoch 36/1000\n",
      "1012/1012 [==============================] - 0s 39us/step - loss: 0.6794 - acc: 0.5721 - val_loss: 0.6946 - val_acc: 0.5512\n",
      "Epoch 37/1000\n",
      "1012/1012 [==============================] - 0s 39us/step - loss: 0.6797 - acc: 0.5741 - val_loss: 0.6945 - val_acc: 0.5512\n",
      "Epoch 38/1000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6793 - acc: 0.5702 - val_loss: 0.6942 - val_acc: 0.5591\n",
      "Epoch 39/1000\n",
      "1012/1012 [==============================] - 0s 31us/step - loss: 0.6790 - acc: 0.5771 - val_loss: 0.6949 - val_acc: 0.5551\n",
      "Epoch 40/1000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6788 - acc: 0.5692 - val_loss: 0.6980 - val_acc: 0.5433\n",
      "Epoch 41/1000\n",
      "1012/1012 [==============================] - 0s 31us/step - loss: 0.6783 - acc: 0.5702 - val_loss: 0.6952 - val_acc: 0.5630\n",
      "Epoch 42/1000\n",
      "1012/1012 [==============================] - 0s 53us/step - loss: 0.6787 - acc: 0.5741 - val_loss: 0.6954 - val_acc: 0.5630\n",
      "Epoch 43/1000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6780 - acc: 0.5781 - val_loss: 0.6950 - val_acc: 0.5394\n",
      "Epoch 44/1000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6778 - acc: 0.5830 - val_loss: 0.6949 - val_acc: 0.5394\n",
      "Epoch 45/1000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6786 - acc: 0.5751 - val_loss: 0.6958 - val_acc: 0.5433\n",
      "Epoch 46/1000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6775 - acc: 0.5791 - val_loss: 0.6990 - val_acc: 0.5591\n",
      "Epoch 47/1000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6773 - acc: 0.5761 - val_loss: 0.6963 - val_acc: 0.5551\n",
      "Epoch 48/1000\n",
      "1012/1012 [==============================] - 0s 37us/step - loss: 0.6771 - acc: 0.5810 - val_loss: 0.6958 - val_acc: 0.5394\n",
      "Epoch 49/1000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6767 - acc: 0.5820 - val_loss: 0.6997 - val_acc: 0.5591\n",
      "Epoch 50/1000\n",
      "1012/1012 [==============================] - 0s 57us/step - loss: 0.6774 - acc: 0.5791 - val_loss: 0.6976 - val_acc: 0.5512\n",
      "Epoch 51/1000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6775 - acc: 0.5791 - val_loss: 0.7000 - val_acc: 0.5512\n",
      "Epoch 52/1000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6763 - acc: 0.5899 - val_loss: 0.6963 - val_acc: 0.5433\n",
      "Epoch 53/1000\n",
      "1012/1012 [==============================] - 0s 31us/step - loss: 0.6764 - acc: 0.5840 - val_loss: 0.6968 - val_acc: 0.5433\n",
      "Epoch 54/1000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6759 - acc: 0.5800 - val_loss: 0.6999 - val_acc: 0.5512\n",
      "Epoch 55/1000\n",
      "1012/1012 [==============================] - 0s 35us/step - loss: 0.6760 - acc: 0.5810 - val_loss: 0.6997 - val_acc: 0.5551\n",
      "Epoch 56/1000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6757 - acc: 0.5860 - val_loss: 0.6972 - val_acc: 0.5433\n",
      "Epoch 00056: early stopping\n"
     ]
    }
   ],
   "source": [
    "validation_data_split = 0.2\n",
    "num_epochs = 1000\n",
    "model_batch_size = 128\n",
    "tb_batch_size = 32\n",
    "early_patience = 35\n",
    "\n",
    "tensorboard_cb   = TensorBoard(log_dir='logs', batch_size= tb_batch_size, write_graph= True)\n",
    "earlystopping_cb = EarlyStopping(monitor='val_loss', verbose=1, patience=early_patience, mode='min') \n",
    "processedData =X_hc[:,0:len(X_hc[0])-1]\n",
    "processedLabel = y_hc\n",
    "\n",
    "history = model.fit(processedData\n",
    "                    , processedLabel\n",
    "                    , validation_split=validation_data_split\n",
    "                    , epochs=num_epochs\n",
    "                    , batch_size=model_batch_size\n",
    "                    , callbacks = [tensorboard_cb,earlystopping_cb]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5414012738853503"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(TestData_h_c)\n",
    "predictions[predictions>0.5] = 1 \n",
    "predictions[predictions<0.5]=0\n",
    "#predictions = model.predict(TestData_g_c)\n",
    "accuracy_score(TestDataAct_h_c,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
