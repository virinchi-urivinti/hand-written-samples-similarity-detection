{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "import matplotlib.pyplot \n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import itertools\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "import matplotlib.pyplot\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def concatinate_dataset(df , same_pairs):\n",
    "    dic_img_id={}\n",
    "    keys = df.iloc[:,2:]\n",
    "    for x in range(len(df)):\n",
    "        img_id = df['img_id'][x]\n",
    "        dic_img_id[img_id] = keys.iloc[x].values    \n",
    "    new=[]\n",
    "    for x in range(len(same_pairs)):\n",
    "        key1 = same_pairs['img_id_A'][x]\n",
    "        key2 = same_pairs['img_id_B'][x]\n",
    "        data1 = dic_img_id[key1]\n",
    "        data2 = dic_img_id[key2]\n",
    "        concat = np.concatenate((data1,data2), axis=0)\n",
    "        new.append(concat)\n",
    "    new=pd.DataFrame(np.asarray(new))\n",
    "    data=pd.concat([same_pairs, new], axis=1, sort=False)\n",
    "    return data\n",
    "\n",
    "\n",
    "def difference_dataset(df , same_pairs):\n",
    "    dic_img_id={}\n",
    "    keys = df.iloc[:,2:]\n",
    "    for x in range(len(df)):\n",
    "        img_id = df['img_id'][x]\n",
    "        dic_img_id[img_id] = keys.iloc[x].values    \n",
    "    new=[]\n",
    "    for x in range(len(same_pairs)):\n",
    "        key1 = same_pairs['img_id_A'][x]\n",
    "        key2 = same_pairs['img_id_B'][x]\n",
    "        data1 = dic_img_id[key1]\n",
    "        data2 = dic_img_id[key2]\n",
    "        con = data1 - data2\n",
    "        new.append(con)\n",
    "    new=pd.DataFrame(np.asarray(new))\n",
    "    data=pd.concat([same_pairs, new], axis=1, sort=False)\n",
    "    return data\n",
    "\n",
    "def concat_gsc(df,same_pairs):\n",
    "    dic_img_id={}\n",
    "    keys = df.iloc[:,1:]\n",
    "    for x in range(len(df)):\n",
    "        img_id = df['img_id'][x]\n",
    "        dic_img_id[img_id] = keys.iloc[x].values    \n",
    "    new=[]\n",
    "    for x in range(len(same_pairs)):\n",
    "        key1 = same_pairs['img_id_A'][x]\n",
    "        key2 = same_pairs['img_id_B'][x]\n",
    "        data1 = dic_img_id[key1]\n",
    "        data2 = dic_img_id[key2]\n",
    "        concat = np.concatenate((data1,data2), axis=0)\n",
    "        new.append(concat)\n",
    "    new=pd.DataFrame(np.asarray(new))\n",
    "    data=pd.concat([same_pairs, new], axis=1, sort=False)\n",
    "    return data\n",
    "    \n",
    "\n",
    "def difference_dataset_gsc(df , same_pairs):\n",
    "    dic_img_id={}\n",
    "    keys = df.iloc[:,1:]\n",
    "    v=0\n",
    "    for x in range(len(df)):\n",
    "        v=v+1\n",
    "        img_id = df['img_id'][x]  \n",
    "        dic_img_id[img_id] = keys.iloc[x].values   \n",
    "    new=[]\n",
    "    v=0\n",
    "    for x in range(len(same_pairs)):\n",
    "        v=v+1\n",
    "        key1 = same_pairs['img_id_A'][x]\n",
    "        key2 = same_pairs['img_id_B'][x]\n",
    "        data1 = dic_img_id[key1]\n",
    "        data2 = dic_img_id[key2]\n",
    "        con = data1 - data2\n",
    "        new.append(con)\n",
    "    new=pd.DataFrame(np.asarray(new))\n",
    "    data=pd.concat([same_pairs, new], axis=1, sort=False)\n",
    "    return data    \n",
    "    \n",
    "    \n",
    "def get_target_vector(data_frame):\n",
    "    x = data_frame['target'].values\n",
    "    return x.tolist() \n",
    "\n",
    "def generate_raw_data(df1):\n",
    "    x= df1.drop(['img_id_B', 'img_id_A','target'], axis=1)\n",
    "    return x.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfH = pd.read_csv('H:/ml projects/HumanObserved-Dataset/HumanObserved-Dataset/HumanObserved-Features-Data/HumanObserved-Features-Data.csv')\n",
    "same_pairsH = pd.read_csv('H:/ml projects/HumanObserved-Dataset/HumanObserved-Dataset/HumanObserved-Features-Data/same_pairs.csv')\n",
    "diff_pairsH=pd.read_csv('H:/ml projects/HumanObserved-Dataset/HumanObserved-Dataset/HumanObserved-Features-Data/diffn_pairs.csv')\n",
    "\n",
    "dfG=pd.read_csv('H:/ml projects/GSC-Dataset(1)/GSC-Dataset/GSC-Features-Data/GSC-Features.csv')\n",
    "same_pairsG = pd.read_csv('H:/ml projects/GSC-Dataset(1)/GSC-Dataset/GSC-Features-Data/same_pairs.csv')\n",
    "diff_pairsG = pd.read_csv('H:/ml projects/GSC-Dataset(1)/GSC-Dataset/GSC-Features-Data/diffn_pairs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating human data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_human_con = concatinate_dataset(dfH,same_pairsH)\n",
    "diff_human_con = concatinate_dataset(dfH,diff_pairsH)\n",
    "\n",
    "same_human_subtr= difference_dataset(dfH,same_pairsH)\n",
    "diff_human_subtr= difference_dataset(dfH,diff_pairsH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating GSC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_pairG = diff_pairsG.sample(n=71531)\n",
    "diff_pairG = diff_pairG.reset_index(drop=True)\n",
    "\n",
    "same_gsc_con = concat_gsc(dfG,same_pairsG)\n",
    "diff_gsc_con = concat_gsc(dfG,diff_pairG)\n",
    "\n",
    "same_gsc_subtr= difference_dataset_gsc(dfG,same_pairsG)\n",
    "diff_gsc_subtr= difference_dataset_gsc(dfG,diff_pairG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenated human data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = diff_human_con.sample(n=791)\n",
    "dat=pd.concat([same_human_con,x], axis=0, sort=False)\n",
    "dat2=dat.sample(frac=1)\n",
    "target_human_concat = get_target_vector(dat2)\n",
    "rawdata_human_concat =generate_raw_data(dat2)\n",
    "#np.savetxt('target_hum_concat.csv', target_human_concat, fmt='%.2f', delimiter=',')\n",
    "#np.savetxt('rawdata_hum_concat.csv', rawdata_human_concat, fmt='%.2f', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference of human data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = diff_human_subtr.sample(n=791)\n",
    "dat=pd.concat([same_human_subtr,x], axis=0, sort=False)\n",
    "dat2=dat.sample(frac=1)\n",
    "target_human_subtr = get_target_vector(dat2)\n",
    "rawdata_human_subtr =generate_raw_data(dat2)\n",
    "print(len((rawdata_human_subtr[0])))\n",
    "#np.savetxt('target_hum_subtr.csv', target_human_concat, fmt='%.2f', delimiter=',')\n",
    "#np.savetxt('rawdata_hum_subtr.csv', rawdata_human_concat, fmt='%.2f', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatinated GSC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_gsc_cons = same_gsc_con.sample(frac=0.1).reset_index(drop=True)\n",
    "diff_gsc_cons = diff_gsc_con.sample(frac=0.1).reset_index(drop=True)\n",
    "dat=pd.concat([same_gsc_cons,diff_gsc_cons], axis=0, sort=False)\n",
    "dat2=dat.loc[:, (dat != 0).any(axis=0)]\n",
    "dat2=dat2.sample(frac=1)\n",
    "target_gsc_con = get_target_vector(dat2)\n",
    "rawdata_gsc_con =generate_raw_data(dat2)\n",
    "rawdata_gsc_con.shape\n",
    "#np.savetxt('target_gsc_con.csv', target_gsc_con, fmt='%.2f', delimiter=',')\n",
    "#np.savetxt('rawdata_gsc_con.csv', rawdata_gsc_con, fmt='%.2f', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference GSC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_gsc_sub = same_gsc_subtr.sample(frac=0.1).reset_index(drop=True)\n",
    "diff_gsc_sub = diff_gsc_subtr.sample(frac=0.1).reset_index(drop=True)\n",
    "dat=pd.concat([same_gsc_sub,diff_gsc_sub], axis=0, sort=False)\n",
    "dat2=dat.loc[:, (dat != 0).any(axis=0)]\n",
    "dat2=dat2.sample(frac=1)\n",
    "\n",
    "\n",
    "#np.savetxt('target_gsc_subtr.csv', target_gsc_subtr, fmt='%.2f', delimiter=',')\n",
    "#np.savetxt('rawdata_gsc_subtr.csv', rawdata_human_subtr, fmt='%.2f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialising the parameters\n",
    "maxAcc = 0.0\n",
    "maxIter = 0\n",
    "C_Lambda = 0.01\n",
    "TrainingPercent = 80\n",
    "ValidationPercent = 10\n",
    "TestPercent = 10\n",
    "M = 10\n",
    "PHI = []\n",
    "IsSynthetic = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTargetVector(reader):\n",
    "    t = []\n",
    "    for row in reader:  \n",
    "        t.append(float(row))\n",
    "    return t\n",
    "#Generate the data matrix containing the features of the data. And also deleting the features having zero variance.\n",
    "def GenerateRawData(reader, IsSynthetic):    \n",
    "    dataMatrix = reader \n",
    "    dataMatrix = np.transpose(dataMatrix)     \n",
    "    #print (\"Data Matrix Generated..\")\n",
    "    return dataMatrix\n",
    "# Preprocessed the data by saperating 80% of data as training data. and appending those target values\n",
    "def GenerateTrainingTarget(rawTraining,TrainingPercent = 80):\n",
    "    TrainingLen = int(math.ceil(len(rawTraining)*(TrainingPercent*0.01)))\n",
    "    t           = rawTraining[:TrainingLen]\n",
    "    #print(str(TrainingPercent) + \"% Training Target Generated..\")\n",
    "    return t\n",
    "# generate the data matrix for the training data.\n",
    "def GenerateTrainingDataMatrix(rawData, TrainingPercent = 80):\n",
    "    T_len = int(math.ceil(len(rawData[0])*0.01*TrainingPercent))\n",
    "    d2 = rawData[:,0:T_len]\n",
    "    #print(str(TrainingPercent) + \"% Training Data Generated..\")\n",
    "    return d2\n",
    "# generate the data matrix for the validation data.\n",
    "def GenerateValData(rawData, ValPercent, TrainingCount): \n",
    "    valSize = int(math.ceil(len(rawData[0])*ValPercent*0.01))\n",
    "    V_End = TrainingCount + valSize\n",
    "    dataMatrix = rawData[:,TrainingCount+1:V_End]\n",
    "    return dataMatrix\n",
    "#generate the target values for the validation set.\n",
    "def GenerateValTargetVector(rawData, ValPercent, TrainingCount): \n",
    "    valSize = int(math.ceil(len(rawData)*ValPercent*0.01))\n",
    "    V_End = TrainingCount + valSize\n",
    "    t =rawData[TrainingCount+1:V_End]\n",
    "    #print (str(ValPercent) + \"% Val Target Data Generated..\")\n",
    "    return t\n",
    "#generate the co-variance matrix. which is also the big-sigma.and finding the inverse of it.\n",
    "def GenerateBigSigma(Data, MuMatrix,TrainingPercent,IsSynthetic):\n",
    "    BigSigma    = np.zeros((len(Data),len(Data)))\n",
    "    DataT       = np.transpose(Data)\n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))        \n",
    "    varVect     = []\n",
    "    for i in range(0,len(DataT[0])):\n",
    "        vct = []\n",
    "        for j in range(0,int(TrainingLen)):\n",
    "            vct.append(Data[i][j])    \n",
    "        varVect.append(np.var(vct))\n",
    "    \n",
    "    for j in range(len(Data)):\n",
    "        BigSigma[j][j] = varVect[j]+0.2\n",
    "    if IsSynthetic == True:\n",
    "        BigSigma = np.dot(3,BigSigma)\n",
    "    else:\n",
    "        BigSigma = np.dot(200,BigSigma)\n",
    "    \n",
    "    ##print (\"BigSigma Generated..\")\n",
    "    return BigSigma\n",
    "#in the GetScalar,GetRadialBasisOut and Getphimatrix we find out the phi matrix by applying the gaussian distribution function to different means. \n",
    "def GetScalar(DataRow,MuRow, BigSigInv):  \n",
    "    R = np.subtract(DataRow,MuRow)\n",
    "    T = np.dot(BigSigInv,np.transpose(R))  \n",
    "    L = np.dot(R,T)\n",
    "    return L\n",
    "\n",
    "def GetRadialBasisOut(DataRow,MuRow, BigSigInv):    \n",
    "    phi_x = math.exp(-0.5*GetScalar(DataRow,MuRow,BigSigInv))\n",
    "    return phi_x\n",
    "\n",
    "\n",
    "def GetPhiMatrix(Data, MuMatrix, BigSigma, TrainingPercent = 80):\n",
    "    DataT = np.transpose(Data)\n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))         \n",
    "    PHI = np.zeros((int(TrainingLen),len(MuMatrix))) \n",
    "    BigSigInv = np.linalg.inv(BigSigma)\n",
    "    for  C in range(0,len(MuMatrix)):\n",
    "        for R in range(0,int(TrainingLen)):\n",
    "            PHI[R][C] = GetRadialBasisOut(DataT[R], MuMatrix[C], BigSigInv)\n",
    "    #print (\"PHI Generated..\")\n",
    "    return PHI\n",
    "#obtaining the weights for the closed form solution.\n",
    "def GetWeightsClosedForm(PHI, T, Lambda):\n",
    "    Lambda_I = np.identity(len(PHI[0]))\n",
    "    for i in range(0,len(PHI[0])):\n",
    "        Lambda_I[i][i] = Lambda\n",
    "    PHI_T       = np.transpose(PHI)\n",
    "    PHI_SQR     = np.dot(PHI_T,PHI)\n",
    "    PHI_SQR_LI  = np.add(Lambda_I,PHI_SQR)\n",
    "    PHI_SQR_INV = np.linalg.inv(PHI_SQR_LI)\n",
    "    INTER       = np.dot(PHI_SQR_INV, PHI_T)\n",
    "    W           = np.dot(INTER, T)\n",
    "    ##print (\"Training Weights Generated..\")\n",
    "    return W\n",
    "\n",
    "def GetPhiMatrix(Data, MuMatrix, BigSigma, TrainingPercent = 80):\n",
    "    DataT = np.transpose(Data)\n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))         \n",
    "    PHI = np.zeros((int(TrainingLen),len(MuMatrix))) \n",
    "    BigSigInv = np.linalg.inv(BigSigma)\n",
    "    for  C in range(0,len(MuMatrix)):\n",
    "        for R in range(0,int(TrainingLen)):\n",
    "            PHI[R][C] = GetRadialBasisOut(DataT[R], MuMatrix[C], BigSigInv)\n",
    "    #print (\"PHI Generated..\")\n",
    "    return PHI\n",
    "\n",
    "def GetValTest(VAL_PHI,W):\n",
    "    Y = np.dot(W,np.transpose(VAL_PHI))\n",
    "    ##print (\"Test Out Generated..\")\n",
    "    return Y\n",
    "#finding out the erms values to find out the error.\n",
    "def GetErms(VAL_TEST_OUT,ValDataAct):\n",
    "    sum = 0.0\n",
    "    t=0\n",
    "    accuracy = 0.0\n",
    "    counter = 0\n",
    "    val = 0.0\n",
    "    for i in range (0,len(VAL_TEST_OUT)):\n",
    "        sum = sum + math.pow((ValDataAct[i] - VAL_TEST_OUT[i]),2)\n",
    "        if(int(np.around(VAL_TEST_OUT[i], 0)) == ValDataAct[i]):\n",
    "            counter+=1\n",
    "    accuracy = (float((counter*100))/float(len(VAL_TEST_OUT)))\n",
    "    ##print (\"Accuracy Generated..\")\n",
    "    ##print (\"Validation E_RMS : \" + str(math.sqrt(sum/len(VAL_TEST_OUT))))\n",
    "    return (str(accuracy) + ',' +  str(math.sqrt(sum/len(VAL_TEST_OUT))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RawTarget = GetTargetVector(target_gsc_con)\n",
    "RawData   = GenerateRawData(rawdata_gsc_con,IsSynthetic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingTarget = np.array(GenerateTrainingTarget(RawTarget,TrainingPercent))\n",
    "TrainingData   = GenerateTrainingDataMatrix(RawData,TrainingPercent)\n",
    "print(TrainingTarget.shape)\n",
    "print(TrainingData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValDataAct = np.array(GenerateValTargetVector(RawTarget,ValidationPercent, (len(TrainingTarget))))\n",
    "ValData    = GenerateValData(RawData,ValidationPercent, (len(TrainingTarget)))\n",
    "print(ValDataAct.shape)\n",
    "print(ValData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestDataAct = np.array(GenerateValTargetVector(RawTarget,TestPercent, (len(TrainingTarget)+len(ValDataAct))))\n",
    "TestData = GenerateValData(RawData,TestPercent, (len(TrainingTarget)+len(ValDataAct)))\n",
    "print(ValDataAct.shape)\n",
    "print(ValData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ErmsArr = []\n",
    "AccuracyArr = []\n",
    "#using k means clustering algorithm\n",
    "kmeans = KMeans(n_clusters=M, random_state=0).fit(np.transpose(TrainingData))\n",
    "Mu = kmeans.cluster_centers_\n",
    "\n",
    "BigSigma     = GenerateBigSigma(RawData, Mu, TrainingPercent,IsSynthetic)\n",
    "TRAINING_PHI = GetPhiMatrix(RawData, Mu, BigSigma, TrainingPercent)\n",
    "W            = GetWeightsClosedForm(TRAINING_PHI,TrainingTarget,(C_Lambda)) \n",
    "TEST_PHI     = GetPhiMatrix(TestData, Mu, BigSigma, 100) \n",
    "VAL_PHI      = GetPhiMatrix(ValData, Mu, BigSigma, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Mu.shape)\n",
    "print(BigSigma.shape)\n",
    "print(TRAINING_PHI.shape)\n",
    "print(W.shape)\n",
    "print(VAL_PHI.shape)\n",
    "print(TEST_PHI.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Erms on training, validation and test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TR_TEST_OUT  = GetValTest(TRAINING_PHI,W)\n",
    "VAL_TEST_OUT = GetValTest(VAL_PHI,W)\n",
    "TEST_OUT     = GetValTest(TEST_PHI,W)\n",
    "\n",
    "TrainingAccuracy   = str(GetErms(TR_TEST_OUT,TrainingTarget))\n",
    "ValidationAccuracy = str(GetErms(VAL_TEST_OUT,ValDataAct))\n",
    "TestAccuracy       = str(GetErms(TEST_OUT,TestDataAct))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent solution for Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Now        = np.dot(220, W)\n",
    "La           = 2\n",
    "learningRate = 0.01\n",
    "L_Erms_Val   = []\n",
    "L_Erms_TR    = []\n",
    "L_Erms_Test  = []\n",
    "W_Mat        = []\n",
    "\n",
    "for i in range(0,400):\n",
    "    \n",
    "    #print ('---------Iteration: ' + str(i) + '--------------')\n",
    "    Delta_E_D     = -np.dot((TrainingTarget[i] - np.dot(np.transpose(W_Now),TRAINING_PHI[i])),TRAINING_PHI[i])\n",
    "    La_Delta_E_W  = np.dot(La,W_Now)\n",
    "    Delta_E       = np.add(Delta_E_D,La_Delta_E_W)    \n",
    "    Delta_W       = -np.dot(learningRate,Delta_E)\n",
    "    W_T_Next      = W_Now + Delta_W\n",
    "    W_Now         = W_T_Next\n",
    "    \n",
    "    #-----------------TrainingData Accuracy---------------------#\n",
    "    TR_TEST_OUT   = GetValTest(TRAINING_PHI,W_T_Next) \n",
    "    Erms_TR       = GetErms(TR_TEST_OUT,TrainingTarget)\n",
    "    L_Erms_TR.append(float(Erms_TR.split(',')[1]))\n",
    "    \n",
    "    #-----------------ValidationData Accuracy---------------------#\n",
    "    VAL_TEST_OUT  = GetValTest(VAL_PHI,W_T_Next) \n",
    "    Erms_Val      = GetErms(VAL_TEST_OUT,ValDataAct)\n",
    "    L_Erms_Val.append(float(Erms_Val.split(',')[1]))\n",
    "    \n",
    "    #-----------------TestingData Accuracy---------------------#\n",
    "    TEST_OUT      = GetValTest(TEST_PHI,W_T_Next) \n",
    "    Erms_Test = GetErms(TEST_OUT,TestDataAct)\n",
    "    L_Erms_Test.append(float(Erms_Test.split(',')[1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('----------Gradient Descent Solution--------------------')\n",
    "print (\"E_rms Training   = \" + str(np.around(min(L_Erms_TR),5)))\n",
    "print (\"E_rms Validation = \" + str(np.around(min(L_Erms_Val),5)))\n",
    "print (\"E_rms Testing    = \" + str(np.around(min(L_Erms_Test),5)))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(L_Erms_TR,color='r')\n",
    "plt.ylabel('Erms_TR')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(L_Erms_Val)\n",
    "plt.ylabel('Erms_Val')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(L_Erms_Test,color='g')\n",
    "plt.ylabel('Erms_Test')\n",
    "plt.xlabel('Number of iterations')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
